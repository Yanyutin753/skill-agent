# ===================================
# FastAPI Agent Configuration
# ===================================
# Copy this file to .env and fill in your values

# ===================================
# Application Settings
# ===================================
PROJECT_NAME="FastAPI Agent"
VERSION="0.1.0"
DESCRIPTION="AI Agent with tool execution capabilities via FastAPI"
DEBUG=false

# ===================================
# API Configuration
# ===================================
API_V1_PREFIX="/api/v1"

# CORS Origins (comma-separated list)
# Example: http://localhost:3000,http://localhost:8080,https://example.com
ALLOWED_ORIGINS="http://localhost:3000,http://localhost:8000"

# ===================================
# LLM Configuration (Multi-Provider via LiteLLM)
# ===================================
# Required: Your API key for the LLM service
LLM_API_KEY="your-api-key-here"

# Base URL for the LLM API (optional for standard providers)
# Leave empty for default endpoints, or set for custom/proxy endpoints
# Examples:
#   Anthropic: https://api.anthropic.com
#   OpenAI: https://api.openai.com/v1
#   Custom/Proxy: https://your-proxy.com/v1
LLM_API_BASE=""

# Model name in format: "provider/model"
# The system will auto-detect provider if you omit the prefix (e.g., "gpt-4o" -> "openai/gpt-4o")
#
# Supported formats:
#   Standard: "anthropic/claude-3-5-sonnet-20241022"
#   Auto-detect: "claude-3-5-sonnet-20241022" (will become "anthropic/claude-3-5-sonnet-20241022")
#   Legacy: "openai:gpt-4o" (will convert to "openai/gpt-4o")
#
# Common providers:
#   Anthropic: anthropic/claude-3-5-sonnet-20241022, anthropic/claude-3-opus-20240229
#   OpenAI: openai/gpt-4o, openai/gpt-4-turbo, openai/gpt-3.5-turbo, openai/o1-preview
#   xAI: xai/grok-2-latest, xai/grok-4-fast-reasoning
#   Google: gemini/gemini-1.5-pro, gemini/gemini-1.5-flash
#   Mistral: mistral/mistral-large-latest, mistral/mistral-medium-latest
#   OpenRouter: openrouter/anthropic/claude-3.5-sonnet, openrouter/openai/gpt-4o
#   Azure: azure/your-deployment-name (requires LLM_API_BASE)
#   Custom: openai/your-model-name (with custom LLM_API_BASE)
LLM_MODEL="anthropic/claude-3-5-sonnet-20241022"

# ===================================
# Agent Configuration
# ===================================
# Maximum number of execution steps (1-200)
AGENT_MAX_STEPS=50

# Workspace directory for agent operations
AGENT_WORKSPACE_DIR="./workspace"

# ===================================
# Skills Configuration (Progressive Disclosure)
# ===================================
# Enable Claude Skills support
ENABLE_SKILLS=true

# Skills directory path
SKILLS_DIR="src/fastapi_agent/skills"

# ===================================
# MCP Configuration (Model Context Protocol)
# ===================================
# Enable MCP tool integration
ENABLE_MCP=true

# Path to MCP configuration file
# See mcp.json.example for configuration format
MCP_CONFIG_PATH="mcp.json"

# Exa API Key (for Exa MCP server)
# Get your API key at: https://exa.ai
EXA_API_KEY="your-exa-api-key-here"

# ===================================
# Session Management Configuration
# ===================================
# Enable session management for multi-turn conversations
ENABLE_SESSION=true

# Storage backend: "file", "redis", or "postgres"
SESSION_BACKEND="file"

# --- File Backend (default) ---
# Session storage path (supports ~ for home directory)
SESSION_STORAGE_PATH="~/.fastapi-agent/sessions.json"

# --- Redis Backend ---
# Uncomment and configure for Redis storage (high performance, distributed)
# SESSION_BACKEND="redis"
# SESSION_REDIS_HOST="localhost"
# SESSION_REDIS_PORT=6379
# SESSION_REDIS_DB=0
# SESSION_REDIS_PASSWORD=""

# --- PostgreSQL Backend ---
# Uncomment and configure for PostgreSQL storage (persistent, queryable)
# Uses the same POSTGRES_* settings from RAG configuration
# SESSION_BACKEND="postgres"
# SESSION_POSTGRES_TABLE="agent_sessions"

# --- Common Settings ---
# Maximum age of sessions in days before cleanup (1-365)
SESSION_MAX_AGE_DAYS=7

# Maximum number of runs to keep per session (10-1000)
SESSION_MAX_RUNS_PER_SESSION=100

# Number of recent runs to include in history context (1-20)
SESSION_HISTORY_RUNS=3

# ===================================
# System Prompt (Optional)
# ===================================
# Custom system prompt for the agent
# Note: Use {SKILLS_METADATA} placeholder for skills metadata injection
# SYSTEM_PROMPT="You are a helpful AI assistant with access to tools...\n\n{SKILLS_METADATA}"

# ===================================
# Langfuse Observability (Recommended)
# ===================================
# Langfuse provides production-grade observability with:
# - LLM call tracing (automatic via LiteLLM integration)
# - Agent execution tracing (spans, tool calls)
# - Token usage and cost tracking
# - Web-based dashboard for debugging and analytics
#
# Get your keys at: https://cloud.langfuse.com (or self-host)

# Enable Langfuse tracing (replaces legacy debug logging)
LANGFUSE_ENABLED=false

# Langfuse credentials
LANGFUSE_PUBLIC_KEY="pk-lf-..."
LANGFUSE_SECRET_KEY="sk-lf-..."

# Langfuse host (cloud or self-hosted)
# EU Cloud: https://cloud.langfuse.com
# US Cloud: https://us.cloud.langfuse.com
# Self-hosted: https://your-langfuse.example.com
LANGFUSE_HOST="https://cloud.langfuse.com"

# Sampling rate (0.0-1.0, default 1.0 = 100% of traces)
LANGFUSE_SAMPLE_RATE=1.0

# Flush interval in seconds (1.0-60.0)
LANGFUSE_FLUSH_INTERVAL=5.0

# ===================================
# Legacy Debug Logging (Deprecated)
# ===================================
# Note: When LANGFUSE_ENABLED=true, these settings are ignored.
# Use Langfuse for production observability.

# Enable debug logging to files (for local debugging only)
ENABLE_DEBUG_LOGGING=false

# Run log storage backend: "file" or "redis"
RUN_LOG_BACKEND="file"

# Directory for run log files (for file backend)
RUN_LOG_DIR="./logs"

# Number of days to retain run logs (1-365)
RUN_LOG_RETENTION_DAYS=30
